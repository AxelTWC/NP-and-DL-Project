{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbcc9QS1tnBN"
      },
      "source": [
        "**ASSIGNMENT 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2Eeke4Z_EkW"
      },
      "source": [
        "1. Group Name: 110\n",
        "   Member Names: Axel Tang\n",
        "   Member Student Numbers: 300164095\n",
        "   Report Title: CSI4106 A4 - Classificiation Empirical Study | Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMUnCICdyBbs"
      },
      "source": [
        "**Derived Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-so3pwbPKTDX"
      },
      "source": [
        "This notebook is a starting point for Assignment 4. In this assignment, you will perform a classification empirical study. This notebook will help you to create derived datasets in Section 2 of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "bhFvS3q7Lu0R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (3.5.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python310\\lib\\site-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: setuptools in c:\\python310\\lib\\site-packages (from spacy) (63.2.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python310\\lib\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\python310\\lib\\site-packages (from spacy) (1.24.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->spacy) (2.1.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "#let's start by installing spaCy\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "!pip install spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "aCWgl6PLKTDY"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX9WQWGSwU2D"
      },
      "source": [
        "You have been given a list of datasets in the assignment description. Choose one of the datasets and provide the link below and read the dataset using pandas. You should provide a link to your own Github repository even if you are using a reduced version of a dataset from your TA's repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSyg0jjC1jJa"
      },
      "source": [
        "add description of the dataset and your justification of the choices made to obtain the derived datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "1Xx4qMCLKTDb"
      },
      "outputs": [],
      "source": [
        "#Load the dataset you chose.\n",
        "# Make sure the Notebook can load your dataset, just like previous assignments.\n",
        "\n",
        "# url = 'https://raw.githubusercontent.com/baharin/CSI4106-Assignment4-Datasets/main/reduced_file_cnnnews.csv'\n",
        "# url = 'https://raw.githubusercontent.com/baharin/CSI4106-Assignment4-Datasets/main/reduced_drugsComTest_raw_fiveclasses.csv'\n",
        "#provide the link to the raw version of dataset. You *need* to provide a link to *your own* github repository. DO NOT use the link that is provided as an example.\n",
        "url = 'https://raw.githubusercontent.com/AxelTWC/CSI_AI-Assignment4-CSV/main/reduced_file_cnnnews.csv'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "wg24OUV81Xgm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://raw.githubusercontent.com/AxelTWC/CSI_AI-Assignment4-CSV/main/reduced_file_cnnnews.csv\n"
          ]
        }
      ],
      "source": [
        "print(url)\n",
        "data = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "AQ5nSY1HKTDd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "      <th>Body</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Theme</th>\n",
              "      <th>Link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Candy factory didn't evacuate concerned worker...</td>\n",
              "      <td>An eastern Pennsylvania candy factory didn’t e...</td>\n",
              "      <td>An eastern Pennsylvania candy factory didn’t e...</td>\n",
              "      <td>accident investigations, accidents, accidents,...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/pennsylv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Baltimore police ask for public's help identif...</td>\n",
              "      <td>Two shooters were involved in an attack at Mor...</td>\n",
              "      <td>Two shooters were involved in an attack at Mor...</td>\n",
              "      <td>baltimore, brand safety-nsf crime, brand safet...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/morgan-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>An arrest warrant has been issued for a suspec...</td>\n",
              "      <td>Authorities in Pennsylvania say they have issu...</td>\n",
              "      <td>Authorities in Pennsylvania say they have issu...</td>\n",
              "      <td>arrest warrants, arrests, brand safety-nsf cri...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/josh-kru...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>115 improperly stored human remains found in C...</td>\n",
              "      <td>An investigation into more than 115 bodies fou...</td>\n",
              "      <td>An investigation into more than 115 bodies fou...</td>\n",
              "      <td>brand safety-nsf death, brand safety-nsf sensi...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/colorado...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Bronx day care provider and 2 others indicted ...</td>\n",
              "      <td>A Bronx day care provider, her husband and his...</td>\n",
              "      <td>A Bronx day care provider, her husband and his...</td>\n",
              "      <td>brand safety-nsf crime, brand safety-nsf death...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/05/us/bronx-da...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID                                              Title  \\\n",
              "0   1  Candy factory didn't evacuate concerned worker...   \n",
              "1   2  Baltimore police ask for public's help identif...   \n",
              "2   3  An arrest warrant has been issued for a suspec...   \n",
              "3   4  115 improperly stored human remains found in C...   \n",
              "4   5  Bronx day care provider and 2 others indicted ...   \n",
              "\n",
              "                                         Description  \\\n",
              "0  An eastern Pennsylvania candy factory didn’t e...   \n",
              "1  Two shooters were involved in an attack at Mor...   \n",
              "2  Authorities in Pennsylvania say they have issu...   \n",
              "3  An investigation into more than 115 bodies fou...   \n",
              "4  A Bronx day care provider, her husband and his...   \n",
              "\n",
              "                                                Body  \\\n",
              "0  An eastern Pennsylvania candy factory didn’t e...   \n",
              "1  Two shooters were involved in an attack at Mor...   \n",
              "2  Authorities in Pennsylvania say they have issu...   \n",
              "3  An investigation into more than 115 bodies fou...   \n",
              "4  A Bronx day care provider, her husband and his...   \n",
              "\n",
              "                                            Keywords Theme  \\\n",
              "0  accident investigations, accidents, accidents,...    us   \n",
              "1  baltimore, brand safety-nsf crime, brand safet...    us   \n",
              "2  arrest warrants, arrests, brand safety-nsf cri...    us   \n",
              "3  brand safety-nsf death, brand safety-nsf sensi...    us   \n",
              "4  brand safety-nsf crime, brand safety-nsf death...    us   \n",
              "\n",
              "                                                Link  \n",
              "0  https://edition.cnn.com/2023/10/06/us/pennsylv...  \n",
              "1  https://edition.cnn.com/2023/10/06/us/morgan-s...  \n",
              "2  https://edition.cnn.com/2023/10/06/us/josh-kru...  \n",
              "3  https://edition.cnn.com/2023/10/06/us/colorado...  \n",
              "4  https://edition.cnn.com/2023/10/05/us/bronx-da...  "
            ]
          },
          "execution_count": 244,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3pycllPKTDd"
      },
      "source": [
        "This is where you create the NLP pipeline. load() will download the correct model (English)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "wQtSi8XuKTDe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The system cannot find the file specified.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm #<- Download spacy packages.\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccqFArDyKTDf"
      },
      "source": [
        "Applying the pipeline to every sentences creates a Document where every word is a Token object.\n",
        "\n",
        "Doc: https://spacy.io/api/doc\n",
        "\n",
        "Token: https://spacy.io/api/token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "WcaeMUL2KTDg"
      },
      "outputs": [],
      "source": [
        "#Apply nlp pipeline to the column that has your sentences.\n",
        "data['tokenized'] = data['Title'].apply(nlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "7i6ai1I8KTDg"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "      <th>Body</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Theme</th>\n",
              "      <th>Link</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Candy factory didn't evacuate concerned worker...</td>\n",
              "      <td>An eastern Pennsylvania candy factory didn’t e...</td>\n",
              "      <td>An eastern Pennsylvania candy factory didn’t e...</td>\n",
              "      <td>accident investigations, accidents, accidents,...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/pennsylv...</td>\n",
              "      <td>(Candy, factory, did, n't, evacuate, concerned...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Baltimore police ask for public's help identif...</td>\n",
              "      <td>Two shooters were involved in an attack at Mor...</td>\n",
              "      <td>Two shooters were involved in an attack at Mor...</td>\n",
              "      <td>baltimore, brand safety-nsf crime, brand safet...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/morgan-s...</td>\n",
              "      <td>(Baltimore, police, ask, for, public, 's, help...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>An arrest warrant has been issued for a suspec...</td>\n",
              "      <td>Authorities in Pennsylvania say they have issu...</td>\n",
              "      <td>Authorities in Pennsylvania say they have issu...</td>\n",
              "      <td>arrest warrants, arrests, brand safety-nsf cri...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/josh-kru...</td>\n",
              "      <td>(An, arrest, warrant, has, been, issued, for, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>115 improperly stored human remains found in C...</td>\n",
              "      <td>An investigation into more than 115 bodies fou...</td>\n",
              "      <td>An investigation into more than 115 bodies fou...</td>\n",
              "      <td>brand safety-nsf death, brand safety-nsf sensi...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/colorado...</td>\n",
              "      <td>(115, improperly, stored, human, remains, foun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Bronx day care provider and 2 others indicted ...</td>\n",
              "      <td>A Bronx day care provider, her husband and his...</td>\n",
              "      <td>A Bronx day care provider, her husband and his...</td>\n",
              "      <td>brand safety-nsf crime, brand safety-nsf death...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/05/us/bronx-da...</td>\n",
              "      <td>(Bronx, day, care, provider, and, 2, others, i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID                                              Title  \\\n",
              "0   1  Candy factory didn't evacuate concerned worker...   \n",
              "1   2  Baltimore police ask for public's help identif...   \n",
              "2   3  An arrest warrant has been issued for a suspec...   \n",
              "3   4  115 improperly stored human remains found in C...   \n",
              "4   5  Bronx day care provider and 2 others indicted ...   \n",
              "\n",
              "                                         Description  \\\n",
              "0  An eastern Pennsylvania candy factory didn’t e...   \n",
              "1  Two shooters were involved in an attack at Mor...   \n",
              "2  Authorities in Pennsylvania say they have issu...   \n",
              "3  An investigation into more than 115 bodies fou...   \n",
              "4  A Bronx day care provider, her husband and his...   \n",
              "\n",
              "                                                Body  \\\n",
              "0  An eastern Pennsylvania candy factory didn’t e...   \n",
              "1  Two shooters were involved in an attack at Mor...   \n",
              "2  Authorities in Pennsylvania say they have issu...   \n",
              "3  An investigation into more than 115 bodies fou...   \n",
              "4  A Bronx day care provider, her husband and his...   \n",
              "\n",
              "                                            Keywords Theme  \\\n",
              "0  accident investigations, accidents, accidents,...    us   \n",
              "1  baltimore, brand safety-nsf crime, brand safet...    us   \n",
              "2  arrest warrants, arrests, brand safety-nsf cri...    us   \n",
              "3  brand safety-nsf death, brand safety-nsf sensi...    us   \n",
              "4  brand safety-nsf crime, brand safety-nsf death...    us   \n",
              "\n",
              "                                                Link  \\\n",
              "0  https://edition.cnn.com/2023/10/06/us/pennsylv...   \n",
              "1  https://edition.cnn.com/2023/10/06/us/morgan-s...   \n",
              "2  https://edition.cnn.com/2023/10/06/us/josh-kru...   \n",
              "3  https://edition.cnn.com/2023/10/06/us/colorado...   \n",
              "4  https://edition.cnn.com/2023/10/05/us/bronx-da...   \n",
              "\n",
              "                                           tokenized  \n",
              "0  (Candy, factory, did, n't, evacuate, concerned...  \n",
              "1  (Baltimore, police, ask, for, public, 's, help...  \n",
              "2  (An, arrest, warrant, has, been, issued, for, ...  \n",
              "3  (115, improperly, stored, human, remains, foun...  \n",
              "4  (Bronx, day, care, provider, and, 2, others, i...  "
            ]
          },
          "execution_count": 247,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHRfZ2uEKTDh"
      },
      "source": [
        "A Token object has many attributes such as part-of-speech (pos_), lemma (lemma_), etc. Take a look at the documentation to see all attributes.\n",
        "\n",
        "The following function is an example on how you can fetch a specific pos tagging from a sentence. We return the lemmatization because we only want the infinitive word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "qw0a_2ySyUo2"
      },
      "outputs": [],
      "source": [
        "#create empty dataframes that will store your derived datasets\n",
        "\n",
        "derived_dataset1 = pd.DataFrame(columns = ['Class', 'pos'])\n",
        "derived_dataset2 = pd.DataFrame(columns=['Class', 'pos-np', 'entities'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "Yeak1tAOKTDi"
      },
      "outputs": [],
      "source": [
        "def get_pos(sentence, wanted_pos): #wanted_pos refers to the desired pos tagging\n",
        "    verbs = []\n",
        "    for token in sentence:\n",
        "        if token.pos_ in wanted_pos:\n",
        "            verbs.append(token.lemma_) # lemma returns a number. lemma_ return a string\n",
        "    return ' '.join(verbs) # return value is as a string and not a list for countVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "147NRzwKKTDj"
      },
      "outputs": [],
      "source": [
        "#As an example, we use the above function to fetch all the verbs. We store this information in our first derived dataset\n",
        "derived_dataset1['pos'] = data['tokenized'].apply(lambda sent : get_pos(sent, ['VERB']))\n",
        "\n",
        "# Make derived_dataset1 to have NOT_VERB and VERB\n",
        "def get_pos(sentence, target_pos):\n",
        "    doc = nlp(sentence)\n",
        "    pos_tags = [token.pos_ for token in doc]\n",
        "    return any(pos in target_pos for pos in pos_tags)\n",
        "derived_dataset1['Class'] = data['Title'].apply(lambda sent: 'VERB' if get_pos(sent, ['VERB']) else 'NOT_VERB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "G_bUg_fVKTDk"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>VERB</td>\n",
              "      <td>evacuate kill find</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VERB</td>\n",
              "      <td>ask identify</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VERB</td>\n",
              "      <td>issue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>VERB</td>\n",
              "      <td>store find say</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>VERB</td>\n",
              "      <td>indict give wrench</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Class                 pos\n",
              "0  VERB  evacuate kill find\n",
              "1  VERB        ask identify\n",
              "2  VERB               issue\n",
              "3  VERB      store find say\n",
              "4  VERB  indict give wrench"
            ]
          },
          "execution_count": 251,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "derived_dataset1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "id": "AuGv-NnfKTDj"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>pos-np</th>\n",
              "      <th>entities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ORG</td>\n",
              "      <td>True</td>\n",
              "      <td>[(OSHA, 93, 97, ORG)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ORG</td>\n",
              "      <td>True</td>\n",
              "      <td>[(Morgan State University, 70, 93, ORG)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DATE</td>\n",
              "      <td>True</td>\n",
              "      <td>[(1-year-old, 67, 77, DATE)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ORG</td>\n",
              "      <td>True</td>\n",
              "      <td>[(Shakur, 17, 23, ORG)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DATE</td>\n",
              "      <td>True</td>\n",
              "      <td>[(Most days, 0, 9, DATE), (June 3, 76, 82, DATE)]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Class  pos-np                                           entities\n",
              "0   ORG    True                              [(OSHA, 93, 97, ORG)]\n",
              "1   ORG    True           [(Morgan State University, 70, 93, ORG)]\n",
              "2  DATE    True                       [(1-year-old, 67, 77, DATE)]\n",
              "3   ORG    True                            [(Shakur, 17, 23, ORG)]\n",
              "4  DATE    True  [(Most days, 0, 9, DATE), (June 3, 76, 82, DATE)]"
            ]
          },
          "execution_count": 252,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Change this line to fetch your desired pos taggings for the second derived dataset\n",
        "#For Derived Dataset 2, you also need to include Named Entities\n",
        "#Below is just an example of obtaining such entities on a specific sentence, but you would do NER\n",
        "#on the dataset of your choice.\n",
        "#You can choose the types of entities (dates, organization, people) that you want,\n",
        "#and then in your derived dataset, just make sure you include these entities separated by spaces (as shown for verbs)\n",
        "#in a previous cell.\n",
        "\n",
        "# sentence = \"apple is looking at buying U.K. startup for $1 billion\"\n",
        "# doc = nlp(sentence)\n",
        "\n",
        "# for ent in doc.ents:\n",
        "#     print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "def get_entities(sentence): #Rework from the code given above by getting entities with their respecting field.\n",
        "    doc = nlp(sentence)\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in ['ORG', 'MONEY', 'DATE']: \n",
        "            entities.append((ent.text, ent.start_char, ent.end_char, ent.label_))\n",
        "    return entities\n",
        "def get_class(entity):\n",
        "    return entity[3] if entity else None #Returning the Entitiy Type and put it into Class.\n",
        "\n",
        "# Initializing derived_dataset 2.\n",
        "derived_dataset2['pos-np'] = data['tokenized'].apply(lambda sent: get_pos(sent, ['NOUN']))\n",
        "derived_dataset2['entities'] = data['tokenized'].apply(get_entities)\n",
        "derived_dataset2['Class'] = derived_dataset2['entities'].apply(lambda entities: get_class(entities[0]) if entities else None)\n",
        "derived_dataset2 = derived_dataset2[derived_dataset2['entities'].apply(len) > 0]\n",
        "derived_dataset2 = derived_dataset2.reset_index(drop=True) #Drop blank entitiy types.\n",
        "derived_dataset2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pX4RgKhKTDk"
      },
      "source": [
        "Now that you have your derived datasets, you can move to perform your classificaton task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GhniwHtzfQt"
      },
      "source": [
        "**Perform Classification Task**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Encode the text as input features with associated values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Encoding text into input features where it removes stopwords and getting tf values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 711)\t0.30151134457776363\n",
            "  (0, 1588)\t0.30151134457776363\n",
            "  (0, 1243)\t0.30151134457776363\n",
            "  (0, 1515)\t0.30151134457776363\n",
            "  (0, 957)\t0.30151134457776363\n",
            "  (0, 4834)\t0.30151134457776363\n",
            "  (0, 3186)\t0.30151134457776363\n",
            "  (0, 1561)\t0.30151134457776363\n",
            "  (0, 2404)\t0.30151134457776363\n",
            "  (0, 3090)\t0.30151134457776363\n",
            "  (0, 1690)\t0.30151134457776363\n",
            "  (1, 427)\t0.2773500981126146\n",
            "  (1, 3282)\t0.2773500981126146\n",
            "  (1, 344)\t0.2773500981126146\n",
            "  (1, 3418)\t0.2773500981126146\n",
            "  (1, 2037)\t0.2773500981126146\n",
            "  (1, 2152)\t0.2773500981126146\n",
            "  (1, 3936)\t0.2773500981126146\n",
            "  (1, 3108)\t0.2773500981126146\n",
            "  (1, 2852)\t0.2773500981126146\n",
            "  (1, 4129)\t0.2773500981126146\n",
            "  (1, 4592)\t0.2773500981126146\n",
            "  (1, 2082)\t0.2773500981126146\n",
            "  (1, 1518)\t0.2773500981126146\n",
            "  (2, 327)\t0.3333333333333333\n",
            "  :\t:\n",
            "  (1408, 1957)\t0.3779644730092272\n",
            "  (1408, 139)\t0.3779644730092272\n",
            "  (1409, 3791)\t0.3779644730092272\n",
            "  (1409, 2948)\t0.3779644730092272\n",
            "  (1409, 2044)\t0.3779644730092272\n",
            "  (1409, 4199)\t0.3779644730092272\n",
            "  (1409, 2577)\t0.3779644730092272\n",
            "  (1409, 1409)\t0.3779644730092272\n",
            "  (1409, 2595)\t0.3779644730092272\n",
            "  (1410, 3791)\t0.35355339059327373\n",
            "  (1410, 3188)\t0.35355339059327373\n",
            "  (1410, 4199)\t0.35355339059327373\n",
            "  (1410, 552)\t0.35355339059327373\n",
            "  (1410, 1409)\t0.35355339059327373\n",
            "  (1410, 2023)\t0.35355339059327373\n",
            "  (1410, 2713)\t0.35355339059327373\n",
            "  (1410, 4023)\t0.35355339059327373\n",
            "  (1411, 4871)\t0.35355339059327373\n",
            "  (1411, 1913)\t0.35355339059327373\n",
            "  (1411, 2572)\t0.35355339059327373\n",
            "  (1411, 1350)\t0.35355339059327373\n",
            "  (1411, 555)\t0.35355339059327373\n",
            "  (1411, 2335)\t0.35355339059327373\n",
            "  (1411, 466)\t0.35355339059327373\n",
            "  (1411, 1265)\t0.35355339059327373\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "stop_words = text.ENGLISH_STOP_WORDS \n",
        "data['Title'] = data['Title'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words])) #Filter out stop words\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_count = count_vectorizer.fit_transform(data['Title'])\n",
        "tf_vectorizer = TfidfVectorizer(use_idf=False)\n",
        "X_tf = tf_vectorizer.fit_transform(data['Title'])\n",
        "print(X_tf) #tf values for references."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Define 2 models using some default parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Logisitc Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'bool' object has no attribute 'lower'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\axel0\\Desktop\\CSI4106\\Assignment 4\\CSI4106_A4_template.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/axel0/Desktop/CSI4106/Assignment%204/CSI4106_A4_template.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y2 \u001b[39m=\u001b[39m derived_dataset2[\u001b[39m'\u001b[39m\u001b[39mClass\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/axel0/Desktop/CSI4106/Assignment%204/CSI4106_A4_template.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/axel0/Desktop/CSI4106/Assignment%204/CSI4106_A4_template.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m X2 \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(X2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/axel0/Desktop/CSI4106/Assignment%204/CSI4106_A4_template.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m X2_train, X2_test, y2_train, y2_test \u001b[39m=\u001b[39m train_test_split(X2, y2, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m) \u001b[39m#Train Test Split Function\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/axel0/Desktop/CSI4106/Assignment%204/CSI4106_A4_template.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model2 \u001b[39m=\u001b[39m LogisticRegression() \u001b[39m#Creating the Logistic Regression Model\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1277\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'lower'"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Derived Data\n",
        "X2 = derived_dataset2['pos-np']\n",
        "y2 = derived_dataset2['Class']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X2 = vectorizer.fit_transform(X2)\n",
        "\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42) #Train Test Split Function\n",
        "\n",
        "model2 = LogisticRegression() #Creating the Logistic Regression Model\n",
        "model2.fit(X2_train, y2_train)\n",
        "y2_pred = model2.predict(X2_test)\n",
        "print(\"Logistic Regression Classification Report for derived_dataset2:\")\n",
        "print(classification_report(y2_test, y2_pred))\n",
        "\n",
        "y = derived_dataset1['Class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Logistic Regression Classification Report :\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Model Classification Report for Derived_dataset1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.54      0.61        35\n",
            "           1       0.00      0.00      0.00         1\n",
            "           2       0.83      0.91      0.87        91\n",
            "\n",
            "    accuracy                           0.80       127\n",
            "   macro avg       0.51      0.48      0.49       127\n",
            "weighted avg       0.79      0.80      0.79       127\n",
            "\n",
            "MLP Model Classification Report for Derived_dataset1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.65      0.49        26\n",
            "           1       0.96      0.90      0.93       257\n",
            "\n",
            "    accuracy                           0.88       283\n",
            "   macro avg       0.68      0.78      0.71       283\n",
            "weighted avg       0.91      0.88      0.89       283\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#Derived Data\n",
        "X = derived_dataset2['pos-np']\n",
        "y = derived_dataset2['Class']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(X)\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Train Test Split Function\n",
        "\n",
        "#Creating MLP Model\n",
        "mlp_classifier = MLPClassifier() #MLPClassifier Model\n",
        "mlp_classifier.fit(X_train, y_train)\n",
        "y_pred = mlp_classifier.predict(X_test)\n",
        "\n",
        "print(\"MLP Model Classification Report for Derived_dataset1:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#Derived Data\n",
        "X = derived_dataset1['pos']\n",
        "y = derived_dataset1['Class']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(X)\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Train Test Split Function\n",
        "\n",
        "#Creating MLP Model\n",
        "mlp_classifier = MLPClassifier() #MLPClassifier Model\n",
        "mlp_classifier.fit(X_train, y_train)\n",
        "y_pred = mlp_classifier.predict(X_test)\n",
        "\n",
        "print(\"MLP Model Classification Report for Derived_dataset1:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Train/Test/Evaluate models on 3 datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Model for Derived_Dataset1:\n",
            "Micro Precision: 0.8987252124645893\n",
            "Micro Recall: 0.8987252124645893\n",
            "Macro Precision: 0.4493626062322946\n",
            "Macro Recall: 0.5\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "y = derived_dataset2['Class'] #Target variable\n",
        "stratified_kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "# Logistic Regression Model Evaluation\n",
        "logistic_precision_micro = []\n",
        "logistic_recall_micro = []\n",
        "logistic_precision_macro = []\n",
        "logistic_recall_macro = []\n",
        "\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    model2.fit(X_train, y_train)\n",
        "    y_pred = model2.predict(X_test)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    logistic_precision_micro.append(precision_micro)\n",
        "    logistic_recall_micro.append(recall_micro)\n",
        "    logistic_precision_macro.append(precision_macro)\n",
        "    logistic_recall_macro.append(recall_macro)\n",
        "\n",
        "# Calculate average precision and recall\n",
        "avg_logistic_precision_micro = sum(logistic_precision_micro) / len(logistic_precision_micro)\n",
        "avg_logistic_recall_micro = sum(logistic_recall_micro) / len(logistic_recall_micro)\n",
        "avg_logistic_precision_macro = sum(logistic_precision_macro) / len(logistic_precision_macro)\n",
        "avg_logistic_recall_macro = sum(logistic_recall_macro) / len(logistic_recall_macro)\n",
        "\n",
        "# Print the results\n",
        "print(\"Logistic Regression Model for Derived_Dataset1:\")\n",
        "print(\"Micro Precision:\", avg_logistic_precision_micro)\n",
        "print(\"Micro Recall:\", avg_logistic_recall_micro)\n",
        "print(\"Macro Precision:\", avg_logistic_precision_macro)\n",
        "print(\"Macro Recall:\", avg_logistic_recall_macro)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [1412, 634]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\axel0\\Desktop\\CSI4106\\Assignment 4\\CSI4106_A4_template.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/axel0/Desktop/CSI4106/Assignment%204/CSI4106_A4_template.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m logistic_precision_macro \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/axel0/Desktop/CSI4106/Assignment%204/CSI4106_A4_template.ipynb#X41sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m logistic_recall_macro \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/axel0/Desktop/CSI4106/Assignment%204/CSI4106_A4_template.ipynb#X41sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_index, test_index \u001b[39min\u001b[39;00m stratified_kfold\u001b[39m.\u001b[39msplit(X, y):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/axel0/Desktop/CSI4106/Assignment%204/CSI4106_A4_template.ipynb#X41sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     X_train, X_test \u001b[39m=\u001b[39m X[train_index], X[test_index]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/axel0/Desktop/CSI4106/Assignment%204/CSI4106_A4_template.ipynb#X41sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     y_train, y_test \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39miloc[train_index], y\u001b[39m.\u001b[39miloc[test_index]\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:367\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, groups\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    344\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \n\u001b[0;32m    346\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[39m        The testing set indices for that split.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m     X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m    368\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(X)\n\u001b[0;32m    369\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m>\u001b[39m n_samples:\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:453\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \n\u001b[0;32m    436\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    452\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[1;32m--> 453\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[0;32m    454\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1412, 634]"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "y = derived_dataset2['Class'] #Target variable\n",
        "stratified_kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "# Logistic Regression Model Evaluation\n",
        "logistic_precision_micro = []\n",
        "logistic_recall_micro = []\n",
        "logistic_precision_macro = []\n",
        "logistic_recall_macro = []\n",
        "\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    model2.fit(X_train, y_train)\n",
        "    y_pred = model2.predict(X_test)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    logistic_precision_micro.append(precision_micro)\n",
        "    logistic_recall_micro.append(recall_micro)\n",
        "    logistic_precision_macro.append(precision_macro)\n",
        "    logistic_recall_macro.append(recall_macro)\n",
        "\n",
        "# Calculate average precision and recall\n",
        "avg_logistic_precision_micro = sum(logistic_precision_micro) / len(logistic_precision_micro)\n",
        "avg_logistic_recall_micro = sum(logistic_recall_micro) / len(logistic_recall_micro)\n",
        "avg_logistic_precision_macro = sum(logistic_precision_macro) / len(logistic_precision_macro)\n",
        "avg_logistic_recall_macro = sum(logistic_recall_macro) / len(logistic_recall_macro)\n",
        "\n",
        "# Print the results\n",
        "print(\"Logistic Regression Model:\")\n",
        "print(\"Micro Precision:\", avg_logistic_precision_micro)\n",
        "print(\"Micro Recall:\", avg_logistic_recall_micro)\n",
        "print(\"Macro Precision:\", avg_logistic_precision_macro)\n",
        "print(\"Macro Recall:\", avg_logistic_recall_macro)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Classifier Model:\n",
            "Micro Precision: 0.7854967757344161\n",
            "Micro Recall: 0.7854967757344161\n",
            "Macro Precision: 0.48835236715800223\n",
            "Macro Recall: 0.47708267443086333\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# MLP Classifier Model Evaluation\n",
        "mlp_precision_micro = []\n",
        "mlp_recall_micro = []\n",
        "mlp_precision_macro = []\n",
        "mlp_recall_macro = []\n",
        "\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    mlp_classifier.fit(X_train, y_train)\n",
        "    y_pred = mlp_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    mlp_precision_micro.append(precision_micro)\n",
        "    mlp_recall_micro.append(recall_micro)\n",
        "    mlp_precision_macro.append(precision_macro)\n",
        "    mlp_recall_macro.append(recall_macro)\n",
        "    \n",
        "# Calculate average precision and recall\n",
        "avg_mlp_precision_micro = sum(mlp_precision_micro) / len(mlp_precision_micro)\n",
        "avg_mlp_recall_micro = sum(mlp_recall_micro) / len(mlp_recall_micro)\n",
        "avg_mlp_precision_macro = sum(mlp_precision_macro) / len(mlp_precision_macro)\n",
        "avg_mlp_recall_macro = sum(mlp_recall_macro) / len(mlp_recall_macro)\n",
        "    \n",
        "print(\"MLP Classifier Model:\")\n",
        "print(\"Micro Precision:\", avg_mlp_precision_micro)\n",
        "print(\"Micro Recall:\", avg_mlp_recall_micro)\n",
        "print(\"Macro Precision:\", avg_mlp_precision_macro)\n",
        "print(\"Macro Recall:\", avg_mlp_recall_macro)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Modify parameters of MLP model and perform train/test/evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Classifier Model w/ New Params:\n",
            "Micro Precision: 0.7649868641031765\n",
            "Micro Recall: 0.7649868641031765\n",
            "Macro Precision: 0.4668494434869018\n",
            "Macro Recall: 0.46512917589130126\n"
          ]
        }
      ],
      "source": [
        "y = derived_dataset2['Class']\n",
        "\n",
        "# StratifiedKFold to ensure class balance in each fold\n",
        "stratified_kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=42) #4 Fold Cross Validation\n",
        "\n",
        "# Test Case 1\n",
        "mlp_new1_precision_micro = []\n",
        "mlp_new1_recall_micro = []\n",
        "mlp_new1_precision_macro = []\n",
        "mlp_new1_recall_macro = []\n",
        "\n",
        "# Modify hidden_layer_sizes and activation parameters\n",
        "# hidden_layer_sizesarray-like of shape(n_layers - 2,), default=(100,)\n",
        "# ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
        "# max_iterint, default=200 | Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations.\n",
        "# random_stateint, RandomState instance, default=None | Determines random number generation for weights and bias initialization, train-test split if early stopping is used, and batch sampling when solver=’sgd’ or ‘adam’.\n",
        "mlp_model_new1 = MLPClassifier(hidden_layer_sizes=(25), activation='logistic', max_iter=500, random_state=42)\n",
        "\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    mlp_model_new1.fit(X_train, y_train)\n",
        "    y_pred = mlp_model_new1.predict(X_test)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    mlp_new1_precision_micro.append(precision_micro)\n",
        "    mlp_new1_recall_micro.append(recall_micro)\n",
        "    mlp_new1_precision_macro.append(precision_macro)\n",
        "    mlp_new1_recall_macro.append(recall_macro)\n",
        "\n",
        "# Calculate average with new parameters\n",
        "avg_mlp_new1_precision_micro = sum(mlp_new1_precision_micro) / len(mlp_new1_precision_micro)\n",
        "avg_mlp_new1_recall_micro = sum(mlp_new1_recall_micro) / len(mlp_new1_recall_micro)\n",
        "avg_mlp_new1_precision_macro = sum(mlp_new1_precision_macro) / len(mlp_new1_precision_macro)\n",
        "avg_mlp_new1_recall_macro = sum(mlp_new1_recall_macro) / len(mlp_new1_recall_macro)\n",
        "\n",
        "print(\"MLP Classifier Model w/ New Params:\")\n",
        "print(\"Micro Precision:\", avg_mlp_new1_precision_micro)\n",
        "print(\"Micro Recall:\", avg_mlp_new1_recall_micro)\n",
        "print(\"Macro Precision:\", avg_mlp_new1_precision_macro)\n",
        "print(\"Macro Recall:\", avg_mlp_new1_recall_macro)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Classifier Model w/ Params 2:\n",
            "Micro Precision: 0.7855266300453785\n",
            "Micro Recall: 0.7855266300453785\n",
            "Macro Precision: 0.49119806119417775\n",
            "Macro Recall: 0.47703186142273324\n"
          ]
        }
      ],
      "source": [
        "# Test Case 2\n",
        "mlp_new2_precision_micro = []\n",
        "mlp_new2_recall_micro = []\n",
        "mlp_new2_precision_macro = []\n",
        "mlp_new2_recall_macro = []\n",
        "\n",
        "stratified_kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=69) #4 Fold Cross Validation\n",
        "\n",
        "# learning_rate_initfloat, default=0.001 | The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’.\n",
        "# max_iterint, default=200 | Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations.\n",
        "# random_stateint, RandomState instance, default=None | Determines random number generation for weights and bias initialization, train-test split if early stopping is used, and batch sampling when solver=’sgd’ or ‘adam’.\n",
        "mlp_model_new2 = MLPClassifier(learning_rate_init=0.001, max_iter=420, random_state=69)\n",
        "\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    mlp_model_new2.fit(X_train, y_train)\n",
        "    y_pred = mlp_model_new2.predict(X_test) \n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    mlp_new2_precision_micro.append(precision_micro)\n",
        "    mlp_new2_recall_micro.append(recall_micro)\n",
        "    mlp_new2_precision_macro.append(precision_macro)\n",
        "    mlp_new2_recall_macro.append(recall_macro)\n",
        "\n",
        "# Calculate with new parameters\n",
        "avg_mlp_new2_precision_micro = sum(mlp_new2_precision_micro) / len(mlp_new2_precision_micro)\n",
        "avg_mlp_new2_recall_micro = sum(mlp_new2_recall_micro) / len(mlp_new2_recall_micro)\n",
        "avg_mlp_new2_precision_macro = sum(mlp_new2_precision_macro) / len(mlp_new2_precision_macro)\n",
        "avg_mlp_new2_recall_macro = sum(mlp_new2_recall_macro) / len(mlp_new2_recall_macro)\n",
        "\n",
        "print(\"MLP Classifier Model w/ Params 2:\")\n",
        "print(\"Micro Precision:\", avg_mlp_new2_precision_micro)\n",
        "print(\"Micro Recall:\", avg_mlp_new2_recall_micro)\n",
        "print(\"Macro Precision:\", avg_mlp_new2_precision_macro)\n",
        "print(\"Macro Recall:\", avg_mlp_new2_recall_macro)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logistic Regression Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "        DATE       0.73      0.46      0.56        35\n",
        "       MONEY       0.00      0.00      0.00         1\n",
        "         ORG       0.81      0.93      0.87        91\n",
        "\n",
        "    accuracy                           0.80       127\n",
        "   macro avg       0.51      0.46      0.48       127\n",
        "weighted avg       0.78      0.80      0.78       127\n",
        "\n",
        "MLP Model Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.69      0.51      0.59        35\n",
        "           1       0.00      0.00      0.00         1\n",
        "           2       0.82      0.91      0.86        91\n",
        "\n",
        "    accuracy                           0.80       127\n",
        "   macro avg       0.50      0.48      0.48       127\n",
        "weighted avg       0.78      0.80      0.78       127\n",
        "\n",
        "Logistic Regression Model:\n",
        "Micro Precision: 0.8075690629726933\n",
        "Micro Recall: 0.8075690629726933\n",
        "Macro Precision: 0.5256874461751904\n",
        "Macro Recall: 0.4807136533893943\n",
        "\n",
        "MLP Classifier Model:\n",
        "Micro Precision: 0.7854967757344161\n",
        "Micro Recall: 0.7854967757344161\n",
        "Macro Precision: 0.48835236715800223\n",
        "Macro Recall: 0.47708267443086333\n",
        "\n",
        "MLP Classifier Model w/ New Params:\n",
        "Micro Precision: 0.7649868641031765\n",
        "Micro Recall: 0.7649868641031765\n",
        "Macro Precision: 0.4668494434869018\n",
        "Macro Recall: 0.46512917589130126\n",
        "\n",
        "MLP Classifier Model w/ Params 2:\n",
        "Micro Precision: 0.7855266300453785\n",
        "Micro Recall: 0.7855266300453785\n",
        "Macro Precision: 0.49119806119417775\n",
        "Macro Recall: 0.47703186142273324"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reference:\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "- https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
        "- https://scikit-learn.org/stable/modules/cross_validation.html\n",
        "- https://www.geeksforgeeks.org/python-extracting-rows-using-pandas-iloc/\n",
        "- https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html\n",
        "- https://stackoverflow.com/questions/72348496/my-k-fold-cross-validation-technique-is-giving-error-on-my-dataframe-with-delete\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
