{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbcc9QS1tnBN"
      },
      "source": [
        "**ASSIGNMENT 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2Eeke4Z_EkW"
      },
      "source": [
        "1. Group Name: 110\n",
        "   Member Names: Axel Tang\n",
        "   Member Student Numbers: 300164095\n",
        "   Report Title: CSI4106 A4 - Classificiation Empirical Study | Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMUnCICdyBbs"
      },
      "source": [
        "**Derived Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-so3pwbPKTDX"
      },
      "source": [
        "This notebook is a starting point for Assignment 4. In this assignment, you will perform a classification empirical study. This notebook will help you to create derived datasets in Section 2 of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "bhFvS3q7Lu0R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (3.5.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python310\\lib\\site-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python310\\lib\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\python310\\lib\\site-packages (from spacy) (1.24.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\python310\\lib\\site-packages (from spacy) (63.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\axel0\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->spacy) (2.1.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "#let's start by installing spaCy\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "aCWgl6PLKTDY"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX9WQWGSwU2D"
      },
      "source": [
        "You have been given a list of datasets in the assignment description. Choose one of the datasets and provide the link below and read the dataset using pandas. You should provide a link to your own Github repository even if you are using a reduced version of a dataset from your TA's repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSyg0jjC1jJa"
      },
      "source": [
        "add description of the dataset and your justification of the choices made to obtain the derived datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "1Xx4qMCLKTDb"
      },
      "outputs": [],
      "source": [
        "#Load the dataset you chose.\n",
        "# Make sure the Notebook can load your dataset, just like previous assignments.\n",
        "\n",
        "# url = 'https://raw.githubusercontent.com/baharin/CSI4106-Assignment4-Datasets/main/reduced_file_cnnnews.csv'\n",
        "# url = 'https://raw.githubusercontent.com/baharin/CSI4106-Assignment4-Datasets/main/reduced_drugsComTest_raw_fiveclasses.csv'\n",
        "#provide the link to the raw version of dataset. You *need* to provide a link to *your own* github repository. DO NOT use the link that is provided as an example.\n",
        "url = 'https://raw.githubusercontent.com/AxelTWC/CSI_AI-Assignment4-CSV/main/reduced_file_cnnnews.csv'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "wg24OUV81Xgm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://raw.githubusercontent.com/AxelTWC/CSI_AI-Assignment4-CSV/main/reduced_file_cnnnews.csv\n"
          ]
        }
      ],
      "source": [
        "print(url)\n",
        "data = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "AQ5nSY1HKTDd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "      <th>Body</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Theme</th>\n",
              "      <th>Link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Candy factory didn't evacuate concerned worker...</td>\n",
              "      <td>An eastern Pennsylvania candy factory didn’t e...</td>\n",
              "      <td>An eastern Pennsylvania candy factory didn’t e...</td>\n",
              "      <td>accident investigations, accidents, accidents,...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/pennsylv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Baltimore police ask for public's help identif...</td>\n",
              "      <td>Two shooters were involved in an attack at Mor...</td>\n",
              "      <td>Two shooters were involved in an attack at Mor...</td>\n",
              "      <td>baltimore, brand safety-nsf crime, brand safet...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/morgan-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>An arrest warrant has been issued for a suspec...</td>\n",
              "      <td>Authorities in Pennsylvania say they have issu...</td>\n",
              "      <td>Authorities in Pennsylvania say they have issu...</td>\n",
              "      <td>arrest warrants, arrests, brand safety-nsf cri...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/josh-kru...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>115 improperly stored human remains found in C...</td>\n",
              "      <td>An investigation into more than 115 bodies fou...</td>\n",
              "      <td>An investigation into more than 115 bodies fou...</td>\n",
              "      <td>brand safety-nsf death, brand safety-nsf sensi...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/colorado...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Bronx day care provider and 2 others indicted ...</td>\n",
              "      <td>A Bronx day care provider, her husband and his...</td>\n",
              "      <td>A Bronx day care provider, her husband and his...</td>\n",
              "      <td>brand safety-nsf crime, brand safety-nsf death...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/05/us/bronx-da...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID                                              Title  \\\n",
              "0   1  Candy factory didn't evacuate concerned worker...   \n",
              "1   2  Baltimore police ask for public's help identif...   \n",
              "2   3  An arrest warrant has been issued for a suspec...   \n",
              "3   4  115 improperly stored human remains found in C...   \n",
              "4   5  Bronx day care provider and 2 others indicted ...   \n",
              "\n",
              "                                         Description  \\\n",
              "0  An eastern Pennsylvania candy factory didn’t e...   \n",
              "1  Two shooters were involved in an attack at Mor...   \n",
              "2  Authorities in Pennsylvania say they have issu...   \n",
              "3  An investigation into more than 115 bodies fou...   \n",
              "4  A Bronx day care provider, her husband and his...   \n",
              "\n",
              "                                                Body  \\\n",
              "0  An eastern Pennsylvania candy factory didn’t e...   \n",
              "1  Two shooters were involved in an attack at Mor...   \n",
              "2  Authorities in Pennsylvania say they have issu...   \n",
              "3  An investigation into more than 115 bodies fou...   \n",
              "4  A Bronx day care provider, her husband and his...   \n",
              "\n",
              "                                            Keywords Theme  \\\n",
              "0  accident investigations, accidents, accidents,...    us   \n",
              "1  baltimore, brand safety-nsf crime, brand safet...    us   \n",
              "2  arrest warrants, arrests, brand safety-nsf cri...    us   \n",
              "3  brand safety-nsf death, brand safety-nsf sensi...    us   \n",
              "4  brand safety-nsf crime, brand safety-nsf death...    us   \n",
              "\n",
              "                                                Link  \n",
              "0  https://edition.cnn.com/2023/10/06/us/pennsylv...  \n",
              "1  https://edition.cnn.com/2023/10/06/us/morgan-s...  \n",
              "2  https://edition.cnn.com/2023/10/06/us/josh-kru...  \n",
              "3  https://edition.cnn.com/2023/10/06/us/colorado...  \n",
              "4  https://edition.cnn.com/2023/10/05/us/bronx-da...  "
            ]
          },
          "execution_count": 216,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3pycllPKTDd"
      },
      "source": [
        "This is where you create the NLP pipeline. load() will download the correct model (English)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "wQtSi8XuKTDe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The system cannot find the file specified.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm #<- Download spacy packages.\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccqFArDyKTDf"
      },
      "source": [
        "Applying the pipeline to every sentences creates a Document where every word is a Token object.\n",
        "\n",
        "Doc: https://spacy.io/api/doc\n",
        "\n",
        "Token: https://spacy.io/api/token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "WcaeMUL2KTDg"
      },
      "outputs": [],
      "source": [
        "#Apply nlp pipeline to the column that has your sentences.\n",
        "data['tokenized'] = data['Title'].apply(nlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "7i6ai1I8KTDg"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "      <th>Body</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Theme</th>\n",
              "      <th>Link</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Candy factory didn't evacuate concerned worker...</td>\n",
              "      <td>An eastern Pennsylvania candy factory didn’t e...</td>\n",
              "      <td>An eastern Pennsylvania candy factory didn’t e...</td>\n",
              "      <td>accident investigations, accidents, accidents,...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/pennsylv...</td>\n",
              "      <td>(Candy, factory, did, n't, evacuate, concerned...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Baltimore police ask for public's help identif...</td>\n",
              "      <td>Two shooters were involved in an attack at Mor...</td>\n",
              "      <td>Two shooters were involved in an attack at Mor...</td>\n",
              "      <td>baltimore, brand safety-nsf crime, brand safet...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/morgan-s...</td>\n",
              "      <td>(Baltimore, police, ask, for, public, 's, help...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>An arrest warrant has been issued for a suspec...</td>\n",
              "      <td>Authorities in Pennsylvania say they have issu...</td>\n",
              "      <td>Authorities in Pennsylvania say they have issu...</td>\n",
              "      <td>arrest warrants, arrests, brand safety-nsf cri...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/josh-kru...</td>\n",
              "      <td>(An, arrest, warrant, has, been, issued, for, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>115 improperly stored human remains found in C...</td>\n",
              "      <td>An investigation into more than 115 bodies fou...</td>\n",
              "      <td>An investigation into more than 115 bodies fou...</td>\n",
              "      <td>brand safety-nsf death, brand safety-nsf sensi...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/06/us/colorado...</td>\n",
              "      <td>(115, improperly, stored, human, remains, foun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Bronx day care provider and 2 others indicted ...</td>\n",
              "      <td>A Bronx day care provider, her husband and his...</td>\n",
              "      <td>A Bronx day care provider, her husband and his...</td>\n",
              "      <td>brand safety-nsf crime, brand safety-nsf death...</td>\n",
              "      <td>us</td>\n",
              "      <td>https://edition.cnn.com/2023/10/05/us/bronx-da...</td>\n",
              "      <td>(Bronx, day, care, provider, and, 2, others, i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID                                              Title  \\\n",
              "0   1  Candy factory didn't evacuate concerned worker...   \n",
              "1   2  Baltimore police ask for public's help identif...   \n",
              "2   3  An arrest warrant has been issued for a suspec...   \n",
              "3   4  115 improperly stored human remains found in C...   \n",
              "4   5  Bronx day care provider and 2 others indicted ...   \n",
              "\n",
              "                                         Description  \\\n",
              "0  An eastern Pennsylvania candy factory didn’t e...   \n",
              "1  Two shooters were involved in an attack at Mor...   \n",
              "2  Authorities in Pennsylvania say they have issu...   \n",
              "3  An investigation into more than 115 bodies fou...   \n",
              "4  A Bronx day care provider, her husband and his...   \n",
              "\n",
              "                                                Body  \\\n",
              "0  An eastern Pennsylvania candy factory didn’t e...   \n",
              "1  Two shooters were involved in an attack at Mor...   \n",
              "2  Authorities in Pennsylvania say they have issu...   \n",
              "3  An investigation into more than 115 bodies fou...   \n",
              "4  A Bronx day care provider, her husband and his...   \n",
              "\n",
              "                                            Keywords Theme  \\\n",
              "0  accident investigations, accidents, accidents,...    us   \n",
              "1  baltimore, brand safety-nsf crime, brand safet...    us   \n",
              "2  arrest warrants, arrests, brand safety-nsf cri...    us   \n",
              "3  brand safety-nsf death, brand safety-nsf sensi...    us   \n",
              "4  brand safety-nsf crime, brand safety-nsf death...    us   \n",
              "\n",
              "                                                Link  \\\n",
              "0  https://edition.cnn.com/2023/10/06/us/pennsylv...   \n",
              "1  https://edition.cnn.com/2023/10/06/us/morgan-s...   \n",
              "2  https://edition.cnn.com/2023/10/06/us/josh-kru...   \n",
              "3  https://edition.cnn.com/2023/10/06/us/colorado...   \n",
              "4  https://edition.cnn.com/2023/10/05/us/bronx-da...   \n",
              "\n",
              "                                           tokenized  \n",
              "0  (Candy, factory, did, n't, evacuate, concerned...  \n",
              "1  (Baltimore, police, ask, for, public, 's, help...  \n",
              "2  (An, arrest, warrant, has, been, issued, for, ...  \n",
              "3  (115, improperly, stored, human, remains, foun...  \n",
              "4  (Bronx, day, care, provider, and, 2, others, i...  "
            ]
          },
          "execution_count": 219,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHRfZ2uEKTDh"
      },
      "source": [
        "A Token object has many attributes such as part-of-speech (pos_), lemma (lemma_), etc. Take a look at the documentation to see all attributes.\n",
        "\n",
        "The following function is an example on how you can fetch a specific pos tagging from a sentence. We return the lemmatization because we only want the infinitive word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "qw0a_2ySyUo2"
      },
      "outputs": [],
      "source": [
        "#create empty dataframes that will store your derived datasets\n",
        "\n",
        "derived_dataset1 = pd.DataFrame(columns = ['Class', 'pos'])\n",
        "derived_dataset2 = pd.DataFrame(columns=['Class', 'pos-np', 'entities'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "Yeak1tAOKTDi"
      },
      "outputs": [],
      "source": [
        "def get_pos(sentence, wanted_pos): #wanted_pos refers to the desired pos tagging\n",
        "    verbs = []\n",
        "    for token in sentence:\n",
        "        if token.pos_ in wanted_pos:\n",
        "            verbs.append(token.lemma_) # lemma returns a number. lemma_ return a string\n",
        "    return ' '.join(verbs) # return value is as a string and not a list for countVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "147NRzwKKTDj"
      },
      "outputs": [],
      "source": [
        "#As an example, we use the above function to fetch all the verbs. We store this information in our first derived dataset\n",
        "derived_dataset1['pos'] = data['tokenized'].apply(lambda sent : get_pos(sent, ['VERB']))\n",
        "\n",
        "# Make derived_dataset1 to have NOT_VERB and VERB\n",
        "def get_posforset1(sentence, target_pos):\n",
        "    doc = nlp(sentence)\n",
        "    pos_tags = [token.pos_ for token in doc]\n",
        "    return any(pos in target_pos for pos in pos_tags)\n",
        "derived_dataset1['Class'] = data['Title'].apply(lambda sent: 'VERB' if get_posforset1(sent, ['VERB']) else 'NOT_VERB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "G_bUg_fVKTDk"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>VERB</td>\n",
              "      <td>evacuate kill find</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VERB</td>\n",
              "      <td>ask identify</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VERB</td>\n",
              "      <td>issue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>VERB</td>\n",
              "      <td>store find say</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>VERB</td>\n",
              "      <td>indict give wrench</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Class                 pos\n",
              "0  VERB  evacuate kill find\n",
              "1  VERB        ask identify\n",
              "2  VERB               issue\n",
              "3  VERB      store find say\n",
              "4  VERB  indict give wrench"
            ]
          },
          "execution_count": 223,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "derived_dataset1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "AuGv-NnfKTDj"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>pos-np</th>\n",
              "      <th>entities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ORG</td>\n",
              "      <td>candy factory worker explosion osha</td>\n",
              "      <td>[(OSHA, 93, 97, ORG)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ORG</td>\n",
              "      <td>police public help shooter homecoming event</td>\n",
              "      <td>[(Morgan State University, 70, 93, ORG)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DATE</td>\n",
              "      <td>day care provider other murder charge year dea...</td>\n",
              "      <td>[(1-year-old, 67, 77, DATE)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ORG</td>\n",
              "      <td>killing case history arrest video show</td>\n",
              "      <td>[(Shakur, 17, 23, ORG)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>DATE</td>\n",
              "      <td>day friend</td>\n",
              "      <td>[(Most days, 0, 9, DATE), (June 3, 76, 82, DATE)]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Class                                             pos-np  \\\n",
              "0   ORG                candy factory worker explosion osha   \n",
              "1   ORG        police public help shooter homecoming event   \n",
              "4  DATE  day care provider other murder charge year dea...   \n",
              "6   ORG             killing case history arrest video show   \n",
              "7  DATE                                         day friend   \n",
              "\n",
              "                                            entities  \n",
              "0                              [(OSHA, 93, 97, ORG)]  \n",
              "1           [(Morgan State University, 70, 93, ORG)]  \n",
              "4                       [(1-year-old, 67, 77, DATE)]  \n",
              "6                            [(Shakur, 17, 23, ORG)]  \n",
              "7  [(Most days, 0, 9, DATE), (June 3, 76, 82, DATE)]  "
            ]
          },
          "execution_count": 224,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Change this line to fetch your desired pos taggings for the second derived dataset\n",
        "#For Derived Dataset 2, you also need to include Named Entities\n",
        "#Below is just an example of obtaining such entities on a specific sentence, but you would do NER\n",
        "#on the dataset of your choice.\n",
        "#You can choose the types of entities (dates, organization, people) that you want,\n",
        "#and then in your derived dataset, just make sure you include these entities separated by spaces (as shown for verbs)\n",
        "#in a previous cell.\n",
        "\n",
        "# sentence = \"apple is looking at buying U.K. startup for $1 billion\"\n",
        "# doc = nlp(sentence)\n",
        "\n",
        "# for ent in doc.ents:\n",
        "#     print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "def get_entities(sentence): #Rework from the code given above by getting entities with their respecting field.\n",
        "    doc = nlp(sentence)\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in ['ORG', 'MONEY', 'DATE']: \n",
        "            entities.append((ent.text, ent.start_char, ent.end_char, ent.label_))\n",
        "    return entities\n",
        "def get_class(entity):\n",
        "    return entity[3] if entity else None #Returning the Entitiy Type and put it into Class.\n",
        "\n",
        "# Initializing derived_dataset 2.\n",
        "derived_dataset2['pos-np'] = data['tokenized'].apply(lambda sent: get_pos(sent, ['NOUN']))\n",
        "derived_dataset2['entities'] = data['tokenized'].apply(get_entities)\n",
        "derived_dataset2['Class'] = derived_dataset2['entities'].apply(lambda entities: get_class(entities[0]) if entities else None)\n",
        "derived_dataset2 = derived_dataset2[derived_dataset2['entities'].apply(len) > 0]\n",
        "derived_dataset2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pX4RgKhKTDk"
      },
      "source": [
        "Now that you have your derived datasets, you can move to perform your classificaton task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GhniwHtzfQt"
      },
      "source": [
        "**Perform Classification Task**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Encode the text as input features with associated values\n",
        "    - Encoding text into input features where it removes stopwords and getting tf values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 711)\t0.30151134457776363\n",
            "  (0, 1588)\t0.30151134457776363\n",
            "  (0, 1243)\t0.30151134457776363\n",
            "  (0, 1515)\t0.30151134457776363\n",
            "  (0, 957)\t0.30151134457776363\n",
            "  (0, 4834)\t0.30151134457776363\n",
            "  (0, 3186)\t0.30151134457776363\n",
            "  (0, 1561)\t0.30151134457776363\n",
            "  (0, 2404)\t0.30151134457776363\n",
            "  (0, 3090)\t0.30151134457776363\n",
            "  (0, 1690)\t0.30151134457776363\n",
            "  (1, 427)\t0.2773500981126146\n",
            "  (1, 3282)\t0.2773500981126146\n",
            "  (1, 344)\t0.2773500981126146\n",
            "  (1, 3418)\t0.2773500981126146\n",
            "  (1, 2037)\t0.2773500981126146\n",
            "  (1, 2152)\t0.2773500981126146\n",
            "  (1, 3936)\t0.2773500981126146\n",
            "  (1, 3108)\t0.2773500981126146\n",
            "  (1, 2852)\t0.2773500981126146\n",
            "  (1, 4129)\t0.2773500981126146\n",
            "  (1, 4592)\t0.2773500981126146\n",
            "  (1, 2082)\t0.2773500981126146\n",
            "  (1, 1518)\t0.2773500981126146\n",
            "  (2, 327)\t0.3333333333333333\n",
            "  :\t:\n",
            "  (1408, 1957)\t0.3779644730092272\n",
            "  (1408, 139)\t0.3779644730092272\n",
            "  (1409, 3791)\t0.3779644730092272\n",
            "  (1409, 2948)\t0.3779644730092272\n",
            "  (1409, 2044)\t0.3779644730092272\n",
            "  (1409, 4199)\t0.3779644730092272\n",
            "  (1409, 2577)\t0.3779644730092272\n",
            "  (1409, 1409)\t0.3779644730092272\n",
            "  (1409, 2595)\t0.3779644730092272\n",
            "  (1410, 3791)\t0.35355339059327373\n",
            "  (1410, 3188)\t0.35355339059327373\n",
            "  (1410, 4199)\t0.35355339059327373\n",
            "  (1410, 552)\t0.35355339059327373\n",
            "  (1410, 1409)\t0.35355339059327373\n",
            "  (1410, 2023)\t0.35355339059327373\n",
            "  (1410, 2713)\t0.35355339059327373\n",
            "  (1410, 4023)\t0.35355339059327373\n",
            "  (1411, 4871)\t0.35355339059327373\n",
            "  (1411, 1913)\t0.35355339059327373\n",
            "  (1411, 2572)\t0.35355339059327373\n",
            "  (1411, 1350)\t0.35355339059327373\n",
            "  (1411, 555)\t0.35355339059327373\n",
            "  (1411, 2335)\t0.35355339059327373\n",
            "  (1411, 466)\t0.35355339059327373\n",
            "  (1411, 1265)\t0.35355339059327373\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "stop_words = text.ENGLISH_STOP_WORDS \n",
        "data['Title'] = data['Title'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words])) #Filter out stop words\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_count = count_vectorizer.fit_transform(data['Title'])\n",
        "tf_vectorizer = TfidfVectorizer(use_idf=False)\n",
        "X_tf = tf_vectorizer.fit_transform(data['Title'])\n",
        "print(X_tf) #tf values for references."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Define 2 models using some default parameters\n",
        "    - Logisitc Regression Model\n",
        "    - MLP Classifier Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report For Derived Dataset 1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    NOT_VERB       0.00      0.00      0.00        23\n",
            "        VERB       0.92      1.00      0.96       260\n",
            "\n",
            "    accuracy                           0.92       283\n",
            "   macro avg       0.46      0.50      0.48       283\n",
            "weighted avg       0.84      0.92      0.88       283\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Derived Data\n",
        "X1 = derived_dataset1['pos']\n",
        "y1 = derived_dataset1['Class']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X1 = vectorizer.fit_transform(X1)\n",
        "\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42) #Train Test Split Function\n",
        "\n",
        "model1 = LogisticRegression() #Creating the Logistic Regression Model\n",
        "model1.fit(X1_train, y1_train)\n",
        "y1_pred = model1.predict(X1_test)\n",
        "print(\"Classification Report For Derived Dataset 1:\")\n",
        "print(classification_report(y1_test, y1_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report For Derived Dataset 2:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DATE       0.73      0.46      0.56        35\n",
            "       MONEY       0.00      0.00      0.00         1\n",
            "         ORG       0.81      0.93      0.87        91\n",
            "\n",
            "    accuracy                           0.80       127\n",
            "   macro avg       0.51      0.46      0.48       127\n",
            "weighted avg       0.78      0.80      0.78       127\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Derived Data\n",
        "X2 = derived_dataset2['pos-np']\n",
        "y2 = derived_dataset2['Class']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X2 = vectorizer.fit_transform(X2)\n",
        "\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42) #Train Test Split Function\n",
        "\n",
        "model2 = LogisticRegression() #Creating the Logistic Regression Model\n",
        "model2.fit(X2_train, y2_train)\n",
        "y2_pred = model2.predict(X2_test)\n",
        "print(\"Classification Report For Derived Dataset 2:\")\n",
        "print(classification_report(y2_test, y2_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Model Classification Report for Derived_dataset 1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67        23\n",
            "           1       1.00      0.91      0.95       260\n",
            "\n",
            "    accuracy                           0.92       283\n",
            "   macro avg       0.75      0.96      0.81       283\n",
            "weighted avg       0.96      0.92      0.93       283\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "#Derived Data\n",
        "X = derived_dataset1['pos']\n",
        "y = derived_dataset1['Class']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(X)\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Train Test Split Function\n",
        "\n",
        "#Creating MLP Model\n",
        "mlp_classifier = MLPClassifier() #MLPClassifier Model\n",
        "mlp_classifier.fit(X_train, y_train)\n",
        "y_pred = mlp_classifier.predict(X_test)\n",
        "\n",
        "print(\"MLP Model Classification Report for Derived_dataset 1:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Model Classification Report for Derived_dataset 2:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.60      0.65        35\n",
            "           1       0.00      0.00      0.00         1\n",
            "           2       0.85      0.90      0.87        91\n",
            "\n",
            "    accuracy                           0.81       127\n",
            "   macro avg       0.52      0.50      0.51       127\n",
            "weighted avg       0.80      0.81      0.80       127\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Derived Data\n",
        "X = derived_dataset2['pos-np']\n",
        "y = derived_dataset2['Class']\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(X)\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Train Test Split Function\n",
        "\n",
        "#Creating MLP Model\n",
        "mlp_classifier = MLPClassifier() #MLPClassifier Model\n",
        "mlp_classifier.fit(X_train, y_train)\n",
        "y_pred = mlp_classifier.predict(X_test)\n",
        "\n",
        "print(\"MLP Model Classification Report for Derived_dataset 2:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Train/Test/Evaluate models on 3 datasets\n",
        "    - Use 4-Fold Cross Validation on the Models\n",
        "    - Get the Micro and macro averages on precision and recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Model for Derived_Dataset 1:\n",
            "Micro Precision: 0.9235127478753541\n",
            "Micro Recall: 0.9235127478753541\n",
            "Macro Precision: 0.46175637393767704\n",
            "Macro Recall: 0.5\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "X = derived_dataset1['pos']\n",
        "X = vectorizer.fit_transform(X)\n",
        "y = derived_dataset1['Class'] #Target variable\n",
        "stratified_kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "# Logistic Regression Model\n",
        "logistic_precision_micro = []\n",
        "logistic_recall_micro = []\n",
        "logistic_precision_macro = []\n",
        "logistic_recall_macro = []\n",
        "\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    model1.fit(X_train, y_train)\n",
        "    y_pred = model1.predict(X_test)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    logistic_precision_micro.append(precision_micro)\n",
        "    logistic_recall_micro.append(recall_micro)\n",
        "    logistic_precision_macro.append(precision_macro)\n",
        "    logistic_recall_macro.append(recall_macro)\n",
        "\n",
        "# Calculate average precision and recall\n",
        "avg_logistic_precision_micro = sum(logistic_precision_micro) / len(logistic_precision_micro)\n",
        "avg_logistic_recall_micro = sum(logistic_recall_micro) / len(logistic_recall_micro)\n",
        "avg_logistic_precision_macro = sum(logistic_precision_macro) / len(logistic_precision_macro)\n",
        "avg_logistic_recall_macro = sum(logistic_recall_macro) / len(logistic_recall_macro)\n",
        "\n",
        "# Print the results\n",
        "print(\"Logistic Regression Model for Derived_Dataset 1:\")\n",
        "print(\"Micro Precision:\", avg_logistic_precision_micro)\n",
        "print(\"Micro Recall:\", avg_logistic_recall_micro)\n",
        "print(\"Macro Precision:\", avg_logistic_precision_macro)\n",
        "print(\"Macro Recall:\", avg_logistic_recall_macro)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Model for Derived_Dataset 2:\n",
            "Micro Precision: 0.8075690629726933\n",
            "Micro Recall: 0.8075690629726933\n",
            "Macro Precision: 0.5256874461751904\n",
            "Macro Recall: 0.4807136533893943\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "X = derived_dataset2['pos-np']\n",
        "X = vectorizer.fit_transform(X)\n",
        "y = derived_dataset2['Class'] #Target variable\n",
        "stratified_kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "# Logistic Regression Model\n",
        "logistic_precision_micro = []\n",
        "logistic_recall_micro = []\n",
        "logistic_precision_macro = []\n",
        "logistic_recall_macro = []\n",
        "\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    model2.fit(X_train, y_train)\n",
        "    y_pred = model2.predict(X_test)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    logistic_precision_micro.append(precision_micro)\n",
        "    logistic_recall_micro.append(recall_micro)\n",
        "    logistic_precision_macro.append(precision_macro)\n",
        "    logistic_recall_macro.append(recall_macro)\n",
        "\n",
        "# Calculate average precision and recall\n",
        "avg_logistic_precision_micro = sum(logistic_precision_micro) / len(logistic_precision_micro)\n",
        "avg_logistic_recall_micro = sum(logistic_recall_micro) / len(logistic_recall_micro)\n",
        "avg_logistic_precision_macro = sum(logistic_precision_macro) / len(logistic_precision_macro)\n",
        "avg_logistic_recall_macro = sum(logistic_recall_macro) / len(logistic_recall_macro)\n",
        "\n",
        "# Print the results\n",
        "print(\"Logistic Regression Model for Derived_Dataset 2:\")\n",
        "print(\"Micro Precision:\", avg_logistic_precision_micro)\n",
        "print(\"Micro Recall:\", avg_logistic_recall_micro)\n",
        "print(\"Macro Precision:\", avg_logistic_precision_macro)\n",
        "print(\"Macro Recall:\", avg_logistic_recall_macro)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Classifier Model For Derived Dataset 1:\n",
            "Micro Precision: 0.886685552407932\n",
            "Micro Recall: 0.886685552407932\n",
            "Macro Precision: 0.7023402314898217\n",
            "Macro Recall: 0.9386503067484663\n"
          ]
        }
      ],
      "source": [
        "# MLP Classifier K Fold For Dataset1\n",
        "\n",
        "X = derived_dataset1['pos']\n",
        "X = vectorizer.fit_transform(X)\n",
        "y = derived_dataset1['Class'] #Target variable\n",
        "\n",
        "mlp_precision_micro = []\n",
        "mlp_recall_micro = []\n",
        "mlp_precision_macro = []\n",
        "mlp_recall_macro = []\n",
        "\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    mlp_classifier.fit(X_train, y_train)\n",
        "    y_pred = mlp_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    mlp_precision_micro.append(precision_micro)\n",
        "    mlp_recall_micro.append(recall_micro)\n",
        "    mlp_precision_macro.append(precision_macro)\n",
        "    mlp_recall_macro.append(recall_macro)\n",
        "    \n",
        "# Calculate average precision and recall\n",
        "avg_mlp_precision_micro = sum(mlp_precision_micro) / len(mlp_precision_micro)\n",
        "avg_mlp_recall_micro = sum(mlp_recall_micro) / len(mlp_recall_micro)\n",
        "avg_mlp_precision_macro = sum(mlp_precision_macro) / len(mlp_precision_macro)\n",
        "avg_mlp_recall_macro = sum(mlp_recall_macro) / len(mlp_recall_macro)\n",
        "    \n",
        "print(\"MLP Classifier Model For Derived Dataset 1:\")\n",
        "print(\"Micro Precision:\", avg_mlp_precision_micro)\n",
        "print(\"Micro Recall:\", avg_mlp_recall_micro)\n",
        "print(\"Macro Precision:\", avg_mlp_precision_macro)\n",
        "print(\"Macro Recall:\", avg_mlp_recall_macro)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Classifier Model For Derived Dataset 2:\n",
            "Micro Precision: 0.7681613725021893\n",
            "Micro Recall: 0.7681613725021893\n",
            "Macro Precision: 0.4710093731874106\n",
            "Macro Recall: 0.4716956428717605\n"
          ]
        }
      ],
      "source": [
        "# MLP Classifier K Fold For Dataset 2\n",
        "\n",
        "X = derived_dataset2['pos-np']\n",
        "X = vectorizer.fit_transform(X)\n",
        "y = derived_dataset2['Class'] #Target variable\n",
        "\n",
        "mlp_precision_micro = []\n",
        "mlp_recall_micro = []\n",
        "mlp_precision_macro = []\n",
        "mlp_recall_macro = []\n",
        "\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    mlp_classifier.fit(X_train, y_train)\n",
        "    y_pred = mlp_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    mlp_precision_micro.append(precision_micro)\n",
        "    mlp_recall_micro.append(recall_micro)\n",
        "    mlp_precision_macro.append(precision_macro)\n",
        "    mlp_recall_macro.append(recall_macro)\n",
        "    \n",
        "# Calculate average precision and recall\n",
        "avg_mlp_precision_micro = sum(mlp_precision_micro) / len(mlp_precision_micro)\n",
        "avg_mlp_recall_micro = sum(mlp_recall_micro) / len(mlp_recall_micro)\n",
        "avg_mlp_precision_macro = sum(mlp_precision_macro) / len(mlp_precision_macro)\n",
        "avg_mlp_recall_macro = sum(mlp_recall_macro) / len(mlp_recall_macro)\n",
        "    \n",
        "print(\"MLP Classifier Model For Derived Dataset 2:\")\n",
        "print(\"Micro Precision:\", avg_mlp_precision_micro)\n",
        "print(\"Micro Recall:\", avg_mlp_recall_micro)\n",
        "print(\"Macro Precision:\", avg_mlp_precision_macro)\n",
        "print(\"Macro Recall:\", avg_mlp_recall_macro)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Modify parameters of MLP model and perform train/test/evaluate.\n",
        "    - Examples of modifications can be changing the hidden layer size, activation function or learning rate. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Classifier Model w/ New Params for Dataset 1:\n",
            "Micro Precision: 0.8980169971671388\n",
            "Micro Recall: 0.8980169971671388\n",
            "Macro Precision: 0.7149628877952303\n",
            "Macro Recall: 0.9447852760736196\n"
          ]
        }
      ],
      "source": [
        "#For Dataset 1 \n",
        "\n",
        "X = derived_dataset1['pos']\n",
        "X = vectorizer.fit_transform(X)\n",
        "y = derived_dataset1['Class'] #Target variable\n",
        "\n",
        "# StratifiedKFold to ensure class balance in each fold\n",
        "stratified_kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=42) #4 Fold Cross Validation\n",
        "\n",
        "# Test Case 1\n",
        "mlp_new1_precision_micro = []\n",
        "mlp_new1_recall_micro = []\n",
        "mlp_new1_precision_macro = []\n",
        "mlp_new1_recall_macro = []\n",
        "\n",
        "# Modify hidden_layer_sizes and activation parameters\n",
        "# hidden_layer_sizesarray-like of shape(n_layers - 2,), default=(100,)\n",
        "# ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
        "# max_iterint, default=200 | Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations.\n",
        "# random_stateint, RandomState instance, default=None | Determines random number generation for weights and bias initialization, train-test split if early stopping is used, and batch sampling when solver=’sgd’ or ‘adam’.\n",
        "mlp_model_new1 = MLPClassifier(hidden_layer_sizes=(25), activation='logistic', max_iter=500, random_state=42)\n",
        "\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    mlp_model_new1.fit(X_train, y_train)\n",
        "    y_pred = mlp_model_new1.predict(X_test)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    mlp_new1_precision_micro.append(precision_micro)\n",
        "    mlp_new1_recall_micro.append(recall_micro)\n",
        "    mlp_new1_precision_macro.append(precision_macro)\n",
        "    mlp_new1_recall_macro.append(recall_macro)\n",
        "\n",
        "# Calculate average with new parameters\n",
        "avg_mlp_new1_precision_micro = sum(mlp_new1_precision_micro) / len(mlp_new1_precision_micro)\n",
        "avg_mlp_new1_recall_micro = sum(mlp_new1_recall_micro) / len(mlp_new1_recall_micro)\n",
        "avg_mlp_new1_precision_macro = sum(mlp_new1_precision_macro) / len(mlp_new1_precision_macro)\n",
        "avg_mlp_new1_recall_macro = sum(mlp_new1_recall_macro) / len(mlp_new1_recall_macro)\n",
        "\n",
        "print(\"MLP Classifier Model w/ New Params for Dataset 1:\")\n",
        "print(\"Micro Precision:\", avg_mlp_new1_precision_micro)\n",
        "print(\"Micro Recall:\", avg_mlp_new1_recall_micro)\n",
        "print(\"Macro Precision:\", avg_mlp_new1_precision_macro)\n",
        "print(\"Macro Recall:\", avg_mlp_new1_recall_macro)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Classifier Model w/ New Params for Dataset 2:\n",
            "Micro Precision: 0.7649868641031765\n",
            "Micro Recall: 0.7649868641031765\n",
            "Macro Precision: 0.4668494434869018\n",
            "Macro Recall: 0.46512917589130126\n"
          ]
        }
      ],
      "source": [
        "#For Dataset 2\n",
        "X = derived_dataset2['pos-np']\n",
        "X = vectorizer.fit_transform(X)\n",
        "y = derived_dataset2['Class'] #Target variable\n",
        "\n",
        "# StratifiedKFold to ensure class balance in each fold\n",
        "stratified_kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=42) #4 Fold Cross Validation\n",
        "\n",
        "# Test Case 1\n",
        "mlp_new1_precision_micro = []\n",
        "mlp_new1_recall_micro = []\n",
        "mlp_new1_precision_macro = []\n",
        "mlp_new1_recall_macro = []\n",
        "\n",
        "# Modify hidden_layer_sizes and activation parameters\n",
        "# hidden_layer_sizesarray-like of shape(n_layers - 2,), default=(100,)\n",
        "# ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
        "# max_iterint, default=200 | Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations.\n",
        "# random_stateint, RandomState instance, default=None | Determines random number generation for weights and bias initialization, train-test split if early stopping is used, and batch sampling when solver=’sgd’ or ‘adam’.\n",
        "mlp_model_new1 = MLPClassifier(hidden_layer_sizes=(25), activation='logistic', max_iter=500, random_state=42)\n",
        "\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    mlp_model_new1.fit(X_train, y_train)\n",
        "    y_pred = mlp_model_new1.predict(X_test)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    mlp_new1_precision_micro.append(precision_micro)\n",
        "    mlp_new1_recall_micro.append(recall_micro)\n",
        "    mlp_new1_precision_macro.append(precision_macro)\n",
        "    mlp_new1_recall_macro.append(recall_macro)\n",
        "\n",
        "# Calculate average with new parameters\n",
        "avg_mlp_new1_precision_micro = sum(mlp_new1_precision_micro) / len(mlp_new1_precision_micro)\n",
        "avg_mlp_new1_recall_micro = sum(mlp_new1_recall_micro) / len(mlp_new1_recall_micro)\n",
        "avg_mlp_new1_precision_macro = sum(mlp_new1_precision_macro) / len(mlp_new1_precision_macro)\n",
        "avg_mlp_new1_recall_macro = sum(mlp_new1_recall_macro) / len(mlp_new1_recall_macro)\n",
        "\n",
        "print(\"MLP Classifier Model w/ New Params for Dataset 2:\")\n",
        "print(\"Micro Precision:\", avg_mlp_new1_precision_micro)\n",
        "print(\"Micro Recall:\", avg_mlp_new1_recall_micro)\n",
        "print(\"Macro Precision:\", avg_mlp_new1_precision_macro)\n",
        "print(\"Macro Recall:\", avg_mlp_new1_recall_macro)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Classifier Model w/ Params 2 for Dataset 1:\n",
            "Micro Precision: 0.8873937677053825\n",
            "Micro Recall: 0.8873937677053825\n",
            "Macro Precision: 0.7032816377171216\n",
            "Macro Recall: 0.9390337423312883\n"
          ]
        }
      ],
      "source": [
        "# Test Case 2\n",
        "X = derived_dataset1['pos']\n",
        "X = vectorizer.fit_transform(X)\n",
        "y = derived_dataset1['Class'] #Target variable\n",
        "\n",
        "mlp_new2_precision_micro = []\n",
        "mlp_new2_recall_micro = []\n",
        "mlp_new2_precision_macro = []\n",
        "mlp_new2_recall_macro = []\n",
        "\n",
        "stratified_kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=69) #4 Fold Cross Validation\n",
        "\n",
        "# learning_rate_initfloat, default=0.001 | The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’.\n",
        "# max_iterint, default=200 | Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations.\n",
        "# random_stateint, RandomState instance, default=None | Determines random number generation for weights and bias initialization, train-test split if early stopping is used, and batch sampling when solver=’sgd’ or ‘adam’.\n",
        "mlp_model_new2 = MLPClassifier(learning_rate_init=0.001, max_iter=420, random_state=69)\n",
        "\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    mlp_model_new2.fit(X_train, y_train)\n",
        "    y_pred = mlp_model_new2.predict(X_test) \n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    mlp_new2_precision_micro.append(precision_micro)\n",
        "    mlp_new2_recall_micro.append(recall_micro)\n",
        "    mlp_new2_precision_macro.append(precision_macro)\n",
        "    mlp_new2_recall_macro.append(recall_macro)\n",
        "\n",
        "# Calculate with new parameters\n",
        "avg_mlp_new2_precision_micro = sum(mlp_new2_precision_micro) / len(mlp_new2_precision_micro)\n",
        "avg_mlp_new2_recall_micro = sum(mlp_new2_recall_micro) / len(mlp_new2_recall_micro)\n",
        "avg_mlp_new2_precision_macro = sum(mlp_new2_precision_macro) / len(mlp_new2_precision_macro)\n",
        "avg_mlp_new2_recall_macro = sum(mlp_new2_recall_macro) / len(mlp_new2_recall_macro)\n",
        "\n",
        "print(\"MLP Classifier Model w/ Params 2 for Dataset 1:\")\n",
        "print(\"Micro Precision:\", avg_mlp_new2_precision_micro)\n",
        "print(\"Micro Recall:\", avg_mlp_new2_recall_micro)\n",
        "print(\"Macro Precision:\", avg_mlp_new2_precision_macro)\n",
        "print(\"Macro Recall:\", avg_mlp_new2_recall_macro)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Classifier Model w/ Params 2 for Dataset 2:\n",
            "Micro Precision: 0.7855266300453785\n",
            "Micro Recall: 0.7855266300453785\n",
            "Macro Precision: 0.49119806119417775\n",
            "Macro Recall: 0.47703186142273324\n"
          ]
        }
      ],
      "source": [
        "X = derived_dataset2['pos-np']\n",
        "X = vectorizer.fit_transform(X)\n",
        "y = derived_dataset2['Class'] #Target variable\n",
        "\n",
        "\n",
        "mlp_new2_precision_micro = []\n",
        "mlp_new2_recall_micro = []\n",
        "mlp_new2_precision_macro = []\n",
        "mlp_new2_recall_macro = []\n",
        "\n",
        "stratified_kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=69) #4 Fold Cross Validation\n",
        "\n",
        "# learning_rate_initfloat, default=0.001 | The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’.\n",
        "# max_iterint, default=200 | Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations.\n",
        "# random_stateint, RandomState instance, default=None | Determines random number generation for weights and bias initialization, train-test split if early stopping is used, and batch sampling when solver=’sgd’ or ‘adam’.\n",
        "mlp_model_new2 = MLPClassifier(learning_rate_init=0.001, max_iter=420, random_state=69)\n",
        "\n",
        "for train_index, test_index in stratified_kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    mlp_model_new2.fit(X_train, y_train)\n",
        "    y_pred = mlp_model_new2.predict(X_test) \n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    mlp_new2_precision_micro.append(precision_micro)\n",
        "    mlp_new2_recall_micro.append(recall_micro)\n",
        "    mlp_new2_precision_macro.append(precision_macro)\n",
        "    mlp_new2_recall_macro.append(recall_macro)\n",
        "\n",
        "# Calculate with new parameters\n",
        "avg_mlp_new2_precision_micro = sum(mlp_new2_precision_micro) / len(mlp_new2_precision_micro)\n",
        "avg_mlp_new2_recall_micro = sum(mlp_new2_recall_micro) / len(mlp_new2_recall_micro)\n",
        "avg_mlp_new2_precision_macro = sum(mlp_new2_precision_macro) / len(mlp_new2_precision_macro)\n",
        "avg_mlp_new2_recall_macro = sum(mlp_new2_recall_macro) / len(mlp_new2_recall_macro)\n",
        "\n",
        "print(\"MLP Classifier Model w/ Params 2 for Dataset 2:\")\n",
        "print(\"Micro Precision:\", avg_mlp_new2_precision_micro)\n",
        "print(\"Micro Recall:\", avg_mlp_new2_recall_micro)\n",
        "print(\"Macro Precision:\", avg_mlp_new2_precision_macro)\n",
        "print(\"Macro Recall:\", avg_mlp_new2_recall_macro)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Analyze the obtained results\n",
        "Logistic Regression Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "        DATE       0.73      0.46      0.56        35\n",
        "       MONEY       0.00      0.00      0.00         1\n",
        "         ORG       0.81      0.93      0.87        91\n",
        "\n",
        "    accuracy                           0.80       127\n",
        "   macro avg       0.51      0.46      0.48       127\n",
        "weighted avg       0.78      0.80      0.78       127\n",
        "\n",
        "MLP Model Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.69      0.51      0.59        35\n",
        "           1       0.00      0.00      0.00         1\n",
        "           2       0.82      0.91      0.86        91\n",
        "\n",
        "    accuracy                           0.80       127\n",
        "   macro avg       0.50      0.48      0.48       127\n",
        "weighted avg       0.78      0.80      0.78       127\n",
        "\n",
        "Logistic Regression Model:\n",
        "Micro Precision: 0.8075690629726933\n",
        "Micro Recall: 0.8075690629726933\n",
        "Macro Precision: 0.5256874461751904\n",
        "Macro Recall: 0.4807136533893943\n",
        "\n",
        "MLP Classifier Model:\n",
        "Micro Precision: 0.7854967757344161\n",
        "Micro Recall: 0.7854967757344161\n",
        "Macro Precision: 0.48835236715800223\n",
        "Macro Recall: 0.47708267443086333\n",
        "\n",
        "MLP Classifier Model w/ New Params:\n",
        "Micro Precision: 0.7649868641031765\n",
        "Micro Recall: 0.7649868641031765\n",
        "Macro Precision: 0.4668494434869018\n",
        "Macro Recall: 0.46512917589130126\n",
        "\n",
        "MLP Classifier Model w/ Params 2:\n",
        "Micro Precision: 0.7855266300453785\n",
        "Micro Recall: 0.7855266300453785\n",
        "Macro Precision: 0.49119806119417775\n",
        "Macro Recall: 0.47703186142273324"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reference:\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "- https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
        "- https://scikit-learn.org/stable/modules/cross_validation.html\n",
        "- https://www.geeksforgeeks.org/python-extracting-rows-using-pandas-iloc/\n",
        "- https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html\n",
        "- https://stackoverflow.com/questions/72348496/my-k-fold-cross-validation-technique-is-giving-error-on-my-dataframe-with-delete\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
